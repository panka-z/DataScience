{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview : Deploying your ML Model : Paas vs. Iaas\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment of machine learning models is the process for making your models available in production environments, where they can provide predictions to other software systems. It is only once models are deployed to production that they start adding value, making deployment a crucial step. \n",
    "\n",
    "> When it comes to deployments, you need to decide if you’re going to go with a Platform as a Service **(PaaS)** or Infrastructure as a Service **(IaaS)**. \n",
    "\n",
    "> A **PaaS** can be great for prototyping and businesses with lower traffic. Eventually, once the business grows and/or traffic increases, you’re going to move towards IaaS. \n",
    "\n",
    "> w.r.t **IaaS** : There are plenty of solutions from AWS, Google, Microsoft, etc.. . If you’ve never deployed anything before, I’d recommend starting with Flask or Heroku*. (* Django projects are hosted on Heroku for free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your applications are containerized, deployments on most platforms/infrastructure tend to be easier. \n",
    "\n",
    "> Containerization also gives you the option to use a container orchestration platform (Kubernetes is now the standard) to rapidly scale the number of containers as demand shifts.\n",
    "\n",
    "**Be sure that your deployments occur via a Continuous Deployment platform.**\n",
    "\n",
    "An example set of components involved in the whole deployment lifecycle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Paas_vs_Iaas](images/Paas_vs_Iaas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example :\n",
    "--\n",
    "Why and How should I deploy a ML model ?\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following situation:\n",
    "\n",
    "You have built a super cool machine learning model that can predict if a particular transaction is fraudulent or not. Now, a friend of yours is developing an android application for general banking activities and wants to integrate your machine learning model in their application for its super objective.\n",
    "\n",
    "But your friend found out that, you have coded your model in Python while your friend is building his application in Java. So? Won't it be possible to integrate your machine learning model into your friend's application?\n",
    "\n",
    "Fortunately enough, you have the power of APIs. And the above situation is one of the many where the need of turning your machine learning models into APIs is extremely important.(i.e in simple words you are deploying your ML model as WEB API) Many of the industries are now looking for Data Scientists who can do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Options to implement Machine Learning models**\n",
    "\n",
    "Most of the times, the real use of your machine learning model lies at the heart of an intelligent product – that may be a small component of a recommender system or an intelligent chat-bot. These are the times when the barriers seem very difficult to overcome.\n",
    "\n",
    "For example, the majority of the ML practitioners use R/Python for their experiments. But consumers of those ML models would be software engineers who use a completely different technology stack. There are two ways via which this problem can be solved:\n",
    "\n",
    "1. **Rewriting the whole code in the language that the software engineering folks work**. The above seems like a good idea, but the time & energy required to get those intricate models replicated would be utterly waste. Majority of languages like JavaScript, do not have great libraries to perform ML. One would be wise to stay away from it.\n",
    "\n",
    "2. **API-first approach** – Web APIs have made it easy for cross-language applications to work well. If a frontend developer needs to use your ML Model to create an ML powered web application, they would just need to get the URL Endpoint from where the API is being served."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What are APIs?**\n",
    "\n",
    "> Essentially, APIs are very much like web applications, but instead of giving you a nicely styled HTML page, APIs tend to return data in a standard data-exchange format such as JSON, XML, etc. Once a developer has the desired output they can style it whatever the way they want. \n",
    "\n",
    "There are many popular ML APIs as well for example - \n",
    "\n",
    "> IBM Watson's ML API which is capable of the following:\n",
    "\n",
    "1. Machine Translation - Helps translate text in different language pairs.\n",
    "\n",
    "2. Message Resonance – To find out the popularity of a phrase or word with a predetermined audience.\n",
    "\n",
    "3. Question and Answers - This service provides direct answers to the queries that are triggered by primary document sources.\n",
    "\n",
    "4. User Modelling – To make predictions about social characteristics of someone from a given text.\n",
    "\n",
    "> Google Vision API is also an excellent example which provides dedicated services for Computer Vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what happens is a majority of the cloud providers, and smaller machine learning focused companies provide ready-to-use APIs. They cater to the needs of developers/businesses that do not have expertise in ML, who want to implement ML in their processes or product suites.\n",
    "\n",
    "Popular examples of machine learning APIs suited explicitly for web development stuff are DialogFlow, Microsoft's Cognitive Toolkit, TensorFlow.js, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web-app architecture**  --- very important to understand this first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic_ML_deployment_diagram](images/basic_ML_deployment_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The user (on the left here) is using a browser that runs only Javascript, HTML, and CSS. That’s the frontend. \n",
    "\n",
    "\n",
    "2. It can make calls to a backend server to get results, which it then maybe processes and displays. The backend server should respond ASAP to the frontend’s requests; but the backend may need to talk to databases, third party APIs, and microservices. The backend may also produce slow jobs — such as ML jobs.  Hence to always be responsive to the FrontEnd the webserver puts the user request into a Queue, which is picked up by the worker threads and executed. This way we seperate the ML jobs from Web Server logic. \n",
    "\n",
    "**Now, let’s talk distributed web app architecture.**\n",
    "\n",
    "3. In general, we want to run as many backend instances as possible, for scalability. That’s why there are bubbles coming out of ‘server’ in the diagram above; they represent ‘more of these’. So, each instance has to remain stateless: finish handling the HTTP request and exit. Don’t keep anything in memory between requests, because **a client’s first request might go to one server, and a subsequent request to another.**\n",
    "\n",
    "\n",
    "4. It’s bad if we have a long running endpoint: it would tie up one of our servers (say… doing some ML task), leaving it unable to handle other users’ requests. We need to keep the web server responsive and have it hand off long running tasks, with some kind of shared persistence so that when the user checks progress or requests the result, any server can report. Also, jobs, and parts-of-jobs, should be able to be done in parallel by as many workers as there are resources for.\n",
    "\n",
    "\n",
    "5. The answer is a first-in, first-out (FIFO) queue. The backend simply enqueues jobs. Workers pick and process jobs out of the queue, performing training or inference, and storing models or predictions to the database when done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (above) architecture (deploying an ML model) works like this:\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Backend server receives a request from user’s web browser. It’s in JSON format but semantically would be something like: “Tomorrow is Wednesday and we sold 10 units today. **How many customer support calls should we expect tomorrow?”**\n",
    "\n",
    "2. Backend pushes the job {Wednesday, 10} into a queue (some place decoupled from the backend itself). The queue replies with “Thanks, let’s refer to that as Job ID 562”.\n",
    "\n",
    "3. Backend replies to the user: “I’ll do that calculation. It has ID 562. Please wait”. **Backend is then free to serve other users.**\n",
    "\n",
    "4. The user’s web browser starts displaying a ‘please wait’ spinner.\n",
    "\n",
    "5. Workers — at least, ones that are not currently processing another job — are constantly polling the queue for jobs. Probably, the workers exist on another server/computer, but they can also be different threads/processes on the same computer. Workers might have GPUs, whereas the backend server probably does not need to.\n",
    "\n",
    "6. Eventually, a worker will pick up the job, removing it from the queue, and process it (e.g. run {Wednesday, 10} through the ML model). It’ll save the prediction to a database. Imagine this step takes 5 minutes.\n",
    "\n",
    "7. Meanwhile, the user’s web browser is polling the backend every 30 seconds to ask if job 562 is done yet. The backend checks if the database has a result stored at id=562 and replies accordingly.\n",
    "\n",
    "8. After five minutes plus a bit, the user polls for a result, and we are able to serve it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally : How to implement all this stuff ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask - A web services' framework in Python:\n",
    "---\n",
    "\n",
    "Web service is a form of API only that assumes that an API is hosted over a server and can be consumed. Web API, Web Service - these terms are generally used interchangeably.\n",
    "\n",
    "Coming to Flask, it is a web service development framework in Python. It is not the only one in Python, there couple others as well such as Django, Falcon, Hug, etc.\n",
    "\n",
    "> Note : Flask is easiest to start delopment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Major Steps :**\n",
    "\n",
    "1. pip install flask\n",
    "\n",
    "\n",
    "2. According to your business problem , code your ML model. Say you want to predict weather(Regression problem) or classify mails as good or spam (Classification problem).\n",
    "\n",
    "\n",
    "3. You will now save(means persist) this model. Technically speaking, you will serialize this model. In Python, you call this **Pickling.**\n",
    "\n",
    "> **from sklearn.externals import joblib**\n",
    "\n",
    "> **joblib.dump(lr, 'model.pickle')**\n",
    "\n",
    "> **this saves the lr ML model to a file called model.pickle**\n",
    "\n",
    "4. The Logistic Regression model is now persisted. You can load this model into memory with a single line of code. Loading the model back into your workspace is known as Deserialization.\n",
    "\n",
    "> lr = joblib.load('model.pickle')\n",
    "\n",
    "5. Now, use Flask to serve your persisted model. You will do the following two things:\n",
    "\n",
    "**a. Load the already persisted model into memory when the application starts.**\n",
    "\n",
    "**b. Create an API endpoint that takes input variables, transforms them into the appropriate format, and returns predictions.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Age': 85, 'Sex': 'male', 'Embarked': 'S'},\n",
       " {'Age': 24, 'Sex': '\"female\"', 'Embarked': 'C'},\n",
       " {'Age': 3, 'Sex': 'male', 'Embarked': 'C'},\n",
       " {'Age': 21, 'Sex': 'male', 'Embarked': 'S'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More specifically, your sample input to the API will look like the following:\n",
    "# assume we are building a classifier on the titantic dataset\n",
    "\n",
    "[\n",
    "    {\"Age\": 85, \"Sex\": \"male\", \"Embarked\": \"S\"},\n",
    "    {\"Age\": 24, \"Sex\": '\"female\"', \"Embarked\": \"C\"},\n",
    "    {\"Age\": 3, \"Sex\": \"male\", \"Embarked\": \"C\"},\n",
    "    {\"Age\": 21, \"Sex\": \"male\", \"Embarked\": \"S\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': [0, 1, 1, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (which is a JSON list of inputs)\n",
    "# and your API will output like the following:\n",
    "\n",
    "{\"prediction\": [0, 1, 1, 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions denote the survival statuses where 0 represents No and 1 represents Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's write a function predict() which will do:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "     json_ = request.json\n",
    "     query_df = pd.DataFrame(json_)\n",
    "     query = pd.get_dummies(query_df)\n",
    "     prediction = lr.predict(query)\n",
    "     return jsonify({'prediction': list(prediction)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainer will discuss each and every step of the above code in class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together:**\n",
    "( Assuming that you are working Titanic dataset )\n",
    "\n",
    "1. You loaded Titanic dataset and selected the four features.\n",
    "\n",
    "\n",
    "2. You did the necessary data preprocessing.\n",
    "\n",
    "\n",
    "3. You built a Logistic Regression classifier and serialized it.\n",
    "\n",
    "\n",
    "4. You also serialized all the columns from training as a solution to the less than expected number of columns is to persist the list of columns from training.\n",
    "\n",
    "\n",
    "5. You then wrote a simple API using Flask that would predict if a person had survived in the shipwreck given there age, sex and embarked information.\n",
    "\n",
    "\n",
    "6. Let's put all the code in one place so that we undertstand better. Also, it is a good programming practice if you separate your ML model code and your Flask API code into separate .py files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py should look like this:\n",
    "\n",
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset in a dataframe object and include only four features as mentioned\n",
    "url = \"TitanticTraining.csv\"\n",
    "df = pd.read_csv(url)\n",
    "include = ['Age', 'Sex', 'Embarked', 'Survived']   # Only four features\n",
    "df_ = df[include]\n",
    "\n",
    "# Data Preprocessing\n",
    "# applying One Hot encoding over all categoricals\n",
    "# .. some code here for finding categoricals\n",
    "df_ohe = pd.get_dummies(df_, columns=categoricals)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "dependent_variable = 'Survived'\n",
    "x = df_ohe[df_ohe.columns.difference([dependent_variable])]\n",
    "y = df_ohe[dependent_variable]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x, y)\n",
    "\n",
    "\n",
    "\n",
    "# Save your model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'model.pickle')\n",
    "print(\"Model dumped!\")\n",
    "\n",
    "\n",
    "# Load the model that you just saved\n",
    "lr = joblib.load('model.pickle')\n",
    "\n",
    "\n",
    "# Saving the data columns from training\n",
    "model_columns = list(x.columns)\n",
    "joblib.dump(model_columns, 'model_columns.pickle')\n",
    "print(\"Models columns dumped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your api.py should look like this:\n",
    "\n",
    "# Dependencies\n",
    "from flask import Flask, request, jsonify\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your API definition\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if lr:\n",
    "        try:\n",
    "            json_ = request.json\n",
    "            print(json_)\n",
    "            query = pd.get_dummies(pd.DataFrame(json_))\n",
    "            query = query.reindex(columns=model_columns, fill_value=0)\n",
    "\n",
    "            prediction = list(lr.predict(query))\n",
    "\n",
    "            return jsonify({'prediction': str(prediction)})\n",
    "\n",
    "        except:\n",
    "\n",
    "            return jsonify({'Error': 'Seems input is incomplete or not in JSON'})\n",
    "    else:\n",
    "        print ('Train the model first')\n",
    "        return ('No model here to use')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        port = int(sys.argv[1]) # This is for a command-line input\n",
    "    except:\n",
    "        port = 12345 # If you don't provide any port the port will be set to 12345\n",
    "\n",
    "    lr = joblib.load(\"model.pickle\") # Load \"model.pkl\"\n",
    "    print ('Model loaded')\n",
    "    model_columns = joblib.load(\"model_columns.pickle\") # Load \"model_columns.pkl\"\n",
    "    print ('Model columns loaded')\n",
    "\n",
    "    app.run(port=port, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could test this API through **a API client called Postman**. Make sure that model.py and api.py are in the same directory and also make sure that you have compiled them both before testing.\n",
    "\n",
    "From **POSTMAN** we would send a **POST**  request to localhost:12345\n",
    "and provide the JSON data as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Age': 85, 'Sex': 'male', 'Embarked': 'S'},\n",
       " {'Age': 24, 'Sex': '\"female\"', 'Embarked': 'C'},\n",
       " {'Age': 3, 'Sex': 'male', 'Embarked': 'C'},\n",
       " {'Age': 21, 'Sex': 'male', 'Embarked': 'S'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'Age': 85, 'Sex': 'male', 'Embarked': 'S'},\n",
    " {'Age': 24, 'Sex': '\"female\"', 'Embarked': 'C'},\n",
    " {'Age': 3, 'Sex': 'male', 'Embarked': 'C'},\n",
    " {'Age': 21, 'Sex': 'male', 'Embarked': 'S'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': [0, 1, 1, 0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (which is a JSON list of inputs)\n",
    "# and your API will outputs following:\n",
    "\n",
    "{\"prediction\": [0, 1, 1, 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References : ( recommended reading)\n",
    "--\n",
    "For Generic Overview see this blog before the Interview :\n",
    "https://christophergs.github.io/machine%20learning/2019/03/17/how-to-deploy-machine-learning-models/\n",
    "\n",
    "Trying to Deploy a simple ML model , see steps here :\n",
    "\n",
    "https://medium.com/analytics-vidhya/how-to-deploy-simple-machine-learning-models-for-free-56cdccc62b8d\n",
    "\n",
    "\n",
    "or here :\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/machine-learning-models-api-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
